# ðŸ¤– RobustWalker

**Blind Locomotion for Unitree Go1 using Deep Reinforcement Learning**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![MuJoCo](https://img.shields.io/badge/MuJoCo-3.0+-green.svg)](https://mujoco.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ðŸ“– Overview

RobustWalker trains a **PPO-based RL policy** to control the Unitree Go1 quadruped robot using **only proprioceptive sensing** (no cameras or LiDAR). The robot learns to walk robustly on rough terrain and recover from external disturbances through **domain randomization** during training.

### ðŸŽ¥ Training Demo

[![Training Demo](https://img.shields.io/badge/â–¶ï¸_Watch_Training_Video-Google_Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)](https://drive.google.com/file/d/1CoJNGUmFYeM_CfP4g9liCz5-LFeYYQjU/view?usp=sharing)

> **[Click here to watch the trained policy in action â†’](https://drive.google.com/file/d/1CoJNGUmFYeM_CfP4g9liCz5-LFeYYQjU/view?usp=sharing)**

### Key Features

- ðŸƒ **Blind Locomotion**: Walks using only joint encoders and IMUâ€”no vision required
- ðŸŒ **Domain Randomization**: Randomizes friction, payload, motor strength, and external pushes for sim-to-real transfer
- âš¡ **Parallel Training**: Vectorized environments for fast training with Stable-Baselines3
- ðŸ“Š **Comprehensive Rewards**: Multi-objective reward function balancing speed, efficiency, and stability

---

## ðŸ—ï¸ Architecture

### Policy Network

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Observation (57-dim)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Joint Positions (12) â”‚ Joint Velocities (12) â”‚ IMU (6)     â”‚
â”‚  Velocity Commands (3) â”‚ Action History (24)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MLP [256, 256]                           â”‚
â”‚                    Activation: ELU                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Joint Position Targets (12)                  â”‚
â”‚              (PD Controller â†’ Torques â†’ Robot)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Observation Space (57 dimensions)

| Component | Dimensions | Description |
|-----------|------------|-------------|
| Joint Positions | 12 | Normalized encoder readings for all leg joints |
| Joint Velocities | 12 | Scaled velocity measurements |
| Base Angular Velocity | 3 | IMU gyroscope in body frame |
| Projected Gravity | 3 | Gravity vector in body frame (detects tilt) |
| Velocity Commands | 3 | Target (vx, vy, Ï‰z) for command tracking |
| Action History | 24 | Last 2 actions for temporal context |

### Action Space (12 dimensions)

Joint position targets for all 12 actuators:
- **FR** (Front Right): hip, thigh, calf
- **FL** (Front Left): hip, thigh, calf  
- **RR** (Rear Right): hip, thigh, calf
- **RL** (Rear Left): hip, thigh, calf

---

## ðŸŽ¯ Reward Function

The reward function balances multiple objectives:

```python
reward = velocity_tracking + alive_bonus 
       - torque_penalty - action_rate_penalty
       - stumble_penalty - orientation_penalty - termination_penalty
```

| Component | Weight | Description |
|-----------|--------|-------------|
| **Velocity Tracking** | 1.0 | Gaussian reward for matching commanded velocity |
| **Alive Bonus** | 0.1 | Small reward for each timestep survived |
| **Torque Penalty** | 0.001 | Minimizes energy consumption |
| **Action Rate Penalty** | 0.1 | Encourages smooth joint motion |
| **Stumble Penalty** | 2.0 | Penalizes body-ground contact |
| **Orientation Penalty** | 0.5 | Keeps robot upright |
| **Termination Penalty** | 5.0 | Large penalty for falling |

---

## ðŸ”€ Domain Randomization

For robust sim-to-real transfer, we randomize:

| Parameter | Range | Applied When |
|-----------|-------|--------------|
| **Ground Friction** | [0.5, 1.2] | Per episode |
| **Payload Mass** | [0, 4] kg | Per episode |
| **Motor Strength** | [0.9, 1.1]Ã— | Per episode |
| **External Pushes** | [0, 15] N | Every 5-10 seconds |

---

## ðŸ“ Project Structure

```
ROBUSTWALKER/
â”œâ”€â”€ assets/go1/              # MuJoCo model files (URDF â†’ MJCF)
â”‚   â”œâ”€â”€ go1.xml              # Robot definition
â”‚   â””â”€â”€ scene.xml            # Scene with ground plane & lighting
â”œâ”€â”€ robustwalker/            # Core Python package
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â”œâ”€â”€ go1_env.py       # Gymnasium environment
â”‚   â”‚   â””â”€â”€ domain_rand.py   # Domain randomization
â”‚   â”œâ”€â”€ rewards/
â”‚   â”‚   â””â”€â”€ locomotion.py    # Multi-objective reward function
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ mujoco_utils.py  # MuJoCo helper functions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # PPO training script
â”‚   â”œâ”€â”€ evaluate.py          # Policy evaluation
â”‚   â””â”€â”€ visualize.py         # Render trained policy
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml         # Hyperparameters
â””â”€â”€ tests/
    â””â”€â”€ test_env.py          # Environment unit tests
```

---

## ðŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/shrirag10/ROBUSTWALKER.git
cd ROBUSTWALKER

# Create virtual environment (recommended)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### Training

```bash
# Train with default config (~2M steps, ~2-3 hours on GPU)
python scripts/train.py

# Custom training
python scripts/train.py --timesteps 5000000 --n-envs 16

# Quick test (10k steps)
python scripts/train.py --test-mode
```

### Evaluation & Visualization

```bash
# Evaluate trained policy
python scripts/evaluate.py --checkpoint logs/best_model.zip

# Visualize in MuJoCo viewer
python scripts/visualize.py --checkpoint logs/best_model.zip

# Record video
python scripts/visualize.py --checkpoint logs/best_model.zip --record
```

---

## âš™ï¸ Configuration

Edit `configs/default.yaml` to customize training:

```yaml
# Training
total_timesteps: 2_000_000
n_envs: 8                    # Parallel environments
learning_rate: 3.0e-4
batch_size: 64

# Policy Network
policy_kwargs:
  net_arch:
    pi: [256, 256]
    vf: [256, 256]
  activation_fn: elu

# Domain Randomization
domain_rand:
  friction_range: [0.5, 1.2]
  payload_range: [0.0, 4.0]
  push_force_range: [0.0, 15.0]
```

---

## ðŸ“ˆ Training Progress

Monitor training with TensorBoard:

```bash
tensorboard --logdir logs/
```

Key metrics to track:
- `rollout/ep_rew_mean` - Average episode reward
- `train/loss` - Policy loss
- `rollout/ep_len_mean` - Episode length (longer = more stable)

---

## ðŸŽ¯ Acceptance Criteria

- [ ] Track 0.8 m/s forward velocity on rough terrain for >15s
- [ ] Recover from 15N lateral push without falling
- [ ] Maintain stable trot gait pattern

---

## ðŸ”¬ Technical Details

### MuJoCo Simulation

- **Physics timestep**: 2ms (500 Hz)
- **Control frequency**: 50 Hz (20ms per action)
- **Episode length**: 1000 steps (20 seconds)

### PPO Hyperparameters

| Parameter | Value |
|-----------|-------|
| Rollout buffer | 2048 steps/env |
| Minibatch size | 64 |
| Epochs per update | 10 |
| Discount (Î³) | 0.99 |
| GAE (Î») | 0.95 |
| Clip range | 0.2 |
| Entropy coefficient | 0.01 |

---

## ðŸ“š References

- [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347) - Schulman et al., 2017
- [Learning to Walk in Minutes](https://arxiv.org/abs/2109.11978) - Rudin et al., 2022
- [Unitree Go1 Documentation](https://www.unitree.com/products/go1)
- [MuJoCo Physics Engine](https://mujoco.org/)

---

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

---

## ðŸ‘¤ Author

**Srinivasan Shriram**  
[GitHub](https://github.com/shrirag10)
